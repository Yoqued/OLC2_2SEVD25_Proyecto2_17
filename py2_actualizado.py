# -*- coding: utf-8 -*-
"""PY2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15LgrzyPaufvUM-K4Tif0K46gItTbvSn7
"""

import pandas as pd
import numpy as np

# Leer el archivo CSV
df = pd.read_csv("data_prueba_proyecto2.csv")

df.info()
df.describe()

df

# 1. TIPO CORRECTO DE LOS DATOS

# Columnas numéricas para el análisis
cols_numericas = [
    'frecuencia_compra',
    'monto_total_gastado',
    'monto_promedio_compra',
    'dias_desde_ultima_compra',
    'antiguedad_cliente_meses',
    'numero_productos_distintos'
]

# Convertir columnas a numérico (errores → NaN)
for col in cols_numericas:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# Convertir columnas a string
df['canal_principal'] = df['canal_principal'].astype("string")
df['texto_reseña'] = df['texto_reseña'].astype("string")
df['producto_categoria'] = df['producto_categoria'].astype("string")

# 2. ELIMINACIÓN DE DUPLICADOS POR cliente_id y reseña_id

df = df.drop_duplicates(subset=[
    'cliente_id',
    'reseña_id'
])

# 3. LIMPIEZA DE VARIABLES

# FRECUENCIA COMPRA

df.loc[df['frecuencia_compra'] < 0, 'frecuencia_compra'] = np.nan

q1 = df['frecuencia_compra'].quantile(0.25)
q3 = df['frecuencia_compra'].quantile(0.75)
iqr = q3 - q1

limite_superior = q3 + 1.5 * iqr

df.loc[df['frecuencia_compra'] > limite_superior, 'frecuencia_compra'] = np.nan

# 3. LIMPIEZA DE VARIABLES

# MONTO TOTAL GASTADO

df.loc[df['monto_total_gastado'] < 0, 'monto_total_gastado'] = np.nan

q1 = df['monto_total_gastado'].quantile(0.25)
q3 = df['monto_total_gastado'].quantile(0.75)
iqr = q3 - q1

limite_superior = q3 + 3.0 * iqr

df.loc[df['monto_total_gastado'] > limite_superior, 'monto_total_gastado'] = np.nan

# 3. LIMPIEZA DE VARIABLES

# MONTO PROMEDIO COMPRA

df.loc[df['monto_promedio_compra'] < 0, 'monto_promedio_compra'] = np.nan

q1 = df['monto_promedio_compra'].quantile(0.25)
q3 = df['monto_promedio_compra'].quantile(0.75)
iqr = q3 - q1

limite_superior = q3 + 1.5 * iqr

df.loc[df['monto_promedio_compra'] > limite_superior, 'monto_promedio_compra'] = np.nan

# 3. LIMPIEZA DE VARIABLES

# DIAS DESDE ULTIMA COMPRA

df.loc[df['dias_desde_ultima_compra'] < 0, 'dias_desde_ultima_compra'] = np.nan

p95 = df["dias_desde_ultima_compra"].quantile(0.95)

df.loc[df["dias_desde_ultima_compra"] > p95, "dias_desde_ultima_compra"] = p95

# 3. LIMPIEZA DE VARIABLES

# ANTIGUEDAD CLIENTE MESES

df.loc[df['antiguedad_cliente_meses'] < 0, 'antiguedad_cliente_meses'] = np.nan

q1 = df['antiguedad_cliente_meses'].quantile(0.25)
q3 = df['antiguedad_cliente_meses'].quantile(0.75)
iqr = q3 - q1

limite_superior = q3 + 3 * iqr

df.loc[
    df['antiguedad_cliente_meses'] > limite_superior,
    'antiguedad_cliente_meses'
] = np.nan

# 3. LIMPIEZA DE VARIABLES

# CANAL PRINCIPAL

df['canal_principal'] = df['canal_principal'].str.strip().str.lower()

# 3. LIMPIEZA DE VARIABLES

# NUMERO PRODUCTOS DISTINTOS

df.loc[df['numero_productos_distintos'] < 0, 'numero_productos_distintos'] = np.nan

# 2. Detección de outliers (IQR - límite superior)
q1 = df['numero_productos_distintos'].quantile(0.25)
q3 = df['numero_productos_distintos'].quantile(0.75)
iqr = q3 - q1

limite_superior = q3 + 1.5 * iqr

df.loc[
    df['numero_productos_distintos'] > limite_superior,
    'numero_productos_distintos'
] = np.nan

# 3. LIMPIEZA DE VARIABLES

# RESEÑA ID

df['reseña_id'] = pd.to_numeric(df['reseña_id'], errors='coerce')

df.loc[df['reseña_id'] <= 0, 'reseña_id'] = np.nan

# 3. LIMPIEZA DE VARIABLES

# TEXTO RESEÑA

df['texto_reseña'] = (
    df['texto_reseña']
    .str.strip()
    .str.lower()
)
df = df[df['texto_reseña'].notna() & (df['texto_reseña'] != "")]

# 3. LIMPIEZA DE VARIABLES

# FECHA RESEÑA

df['fecha_reseña'] = pd.to_datetime(df['fecha_reseña'], errors='coerce')

# 3. LIMPIEZA DE VARIABLES

# PRODUCTO CATEGORIA

df['producto_categoria'] = df['producto_categoria'].str.strip().str.lower()

# 3. LIMPIEZA DE VARIABLES

# LONGITUD RESEÑA

df['longitud_reseña'] = df['texto_reseña'].str.split().str.len()

df['longitud_reseña'] = pd.to_numeric(
    df['longitud_reseña'],
    errors='coerce'
)

# 4. IMPUTACIÓN O ELIMINACIÓN

# CLIENTE ID

df = df[df['cliente_id'].notna()]

# 4. IMPUTACIÓN O ELIMINACIÓN

# FRECUENCIA COMPRA

mediana_frecuencia = df['frecuencia_compra'].median()
df['frecuencia_compra'] = df['frecuencia_compra'].fillna(mediana_frecuencia)

# 4. IMPUTACIÓN O ELIMINACIÓN

# MONTO TOTAL GASTADO

mediana_monto_total = df['monto_total_gastado'].median()
df['monto_total_gastado'] = df['monto_total_gastado'].fillna(mediana_monto_total)

# 4. IMPUTACIÓN O ELIMINACIÓN

# MONTO PROMEDIO COMPRA

mediana_monto_promedio = df['monto_promedio_compra'].median()
df['monto_promedio_compra'] = df['monto_promedio_compra'].fillna(mediana_monto_promedio)

# 4. IMPUTACIÓN O ELIMINACIÓN

# DIAS DESDE ULTIMA COMPRA

mediana_dias = df['dias_desde_ultima_compra'].median()
df['dias_desde_ultima_compra'] = df['dias_desde_ultima_compra'].fillna(mediana_dias)

# 4. IMPUTACION O ELIMINACIÓN

# ANTIGUEDAD CLIENTE MESES

mediana_antiguedad = df['antiguedad_cliente_meses'].median()
df['antiguedad_cliente_meses'] = df['antiguedad_cliente_meses'].fillna(mediana_antiguedad)

# 4. IMPUTACION O ELIMINACIÓN

# NUMERO PRODUCTOS DISTINTOS

mediana_productos = df['numero_productos_distintos'].median()
df['numero_productos_distintos'] = df['numero_productos_distintos'].fillna(mediana_productos)

# 4. IMPUTACION O ELIMINACIÓN

# RESEÑA ID

df = df[df['reseña_id'].notna()]

df.info()
df.describe()

"""# CLUSTERING (NUMÉRICO)"""

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import (
    silhouette_score,
    calinski_harabasz_score,
    davies_bouldin_score
)

# Columnas numéricas que usarás para clustering de clientes
cols_cluster_numerico = [
    'frecuencia_compra',
    'monto_total_gastado',
    'monto_promedio_compra',
    'dias_desde_ultima_compra',
    'antiguedad_cliente_meses',
    'numero_productos_distintos'
]

# Matriz numérica
X_num = df[cols_cluster_numerico].copy()

# Normalización (importante para K-Means)
scaler = StandardScaler()
X_num_scaled = scaler.fit_transform(X_num)

# Elegir K (manual)
k_num = 6

# Elegir (manual)
max_iter_num = 300

kmeans_num = KMeans(
    n_clusters = k_num,
    max_iter = max_iter_num,
    random_state = 42,
    n_init = 10
)

clusters_num = kmeans_num.fit_predict(X_num_scaled)

df['cluster_clientes'] = clusters_num

# Métricas
sil_num = silhouette_score(X_num_scaled, clusters_num)
inercia_num = kmeans_num.inertia_
ch_num = calinski_harabasz_score(X_num_scaled, clusters_num)
db_num = davies_bouldin_score(X_num_scaled, clusters_num)

print("=== CLUSTERING NUMÉRICO (CLIENTES) ===")
print("K (manual) =", k_num)
print("Silhouette:", sil_num)
print("Inercia:", inercia_num)
print("Calinski-Harabasz:", ch_num)
print("Davies-Bouldin:", db_num)

# Tabla de perfil por cluster
perfil_clientes = df.groupby("cluster_clientes")[cols_cluster_numerico].mean()
perfil_clientes

# Tamaño y porcentaje de cluster
cluster_size = (
    df["cluster_clientes"]
    .value_counts()
    .rename("cantidad")
    .to_frame()
)

cluster_size["porcentaje"] = (
    cluster_size["cantidad"] / cluster_size["cantidad"].sum() * 100
)

cluster_size

# Gráfica de barras (ejemplo: gasto total)
import matplotlib.pyplot as plt

df.groupby("cluster_clientes")["monto_total_gastado"].mean().plot(
    kind="bar",
    title="Gasto total promedio por cluster"
)
plt.ylabel("Monto total gastado")
plt.xlabel("Cluster")
plt.show()

import matplotlib.pyplot as plt

df.groupby("cluster_clientes")["dias_desde_ultima_compra"].mean().plot(
    kind="bar",
    title="Días promedio desde la última compra por cluster"
)
plt.ylabel("Días desde la última compra")
plt.xlabel("Cluster")
plt.show()

# Boxplot por variable
plt.figure()
df.boxplot(
    column="frecuencia_compra",
    by="cluster_clientes"
)
plt.title("Distribución de frecuencia de compra por cluster")
plt.suptitle("")
plt.xlabel("Cluster")
plt.ylabel("Frecuencia de compra")
plt.show()

import pandas as pd
import numpy as np

VARS_CLAVE = [
    "monto_total_gastado",
    "frecuencia_compra",
    "dias_desde_ultima_compra",
    "numero_productos_distintos",
]

def etiqueta_por_percentil(valor, p25, p75):
    """3 niveles: bajo / medio / alto (simple y suficiente)"""
    if valor <= p25:
        return "bajo"
    elif valor >= p75:
        return "alto"
    else:
        return "medio"

def interpretar_tamano_cluster(porcentaje):
    """Interpretación simple por porcentaje"""
    if porcentaje >= 30:
        return "Segmento mayoritario"
    elif porcentaje >= 15:
        return "Segmento relevante"
    else:
        return "Segmento minoritario / nicho"

def generar_descripcion_listada_simple(
    df: pd.DataFrame,
    cluster_col: str = "cluster_clientes",
    vars_: list = VARS_CLAVE
):
    # Perfil promedio por cluster
    perfil = df.groupby(cluster_col)[vars_].mean()

    # Umbrales relativos (percentiles entre clusters)
    p25 = perfil.quantile(0.25)
    p75 = perfil.quantile(0.75)

    # Tamaño y porcentaje del cluster
    size = df[cluster_col].value_counts().sort_index()
    pct = (size / size.sum() * 100).round(2)

    bloques = []

    for c in perfil.index:
        row = perfil.loc[c]
        tags = {v: etiqueta_por_percentil(row[v], p25[v], p75[v]) for v in vars_}

        textosA = []

        # 1) Importancia por tamaño
        textosA.append(f"{interpretar_tamano_cluster(float(pct[c]))} ({int(size[c])} clientes, {pct[c]:.2f}%).")

        # 2) Gasto total
        if tags["monto_total_gastado"] == "alto":
            textosA.append("Gasto total alto: clientes de alto valor.")
        elif tags["monto_total_gastado"] == "bajo":
            textosA.append("Gasto total bajo: clientes de menor valor.")
        else:
            textosA.append("Gasto total medio: clientes de valor promedio.")

        # 3) Frecuencia
        if tags["frecuencia_compra"] == "alto":
            textosA.append("Alta frecuencia de compra: tienden a comprar seguido.")
        elif tags["frecuencia_compra"] == "bajo":
            textosA.append("Baja frecuencia de compra: compran esporádicamente.")
        else:
            textosA.append("Frecuencia de compra promedio.")

        # 4) Recencia (invertida: MENOS días = más recientes)
        if tags["dias_desde_ultima_compra"] == "alto":
            textosA.append("Menos activos: ha pasado más tiempo desde su última compra.")
        elif tags["dias_desde_ultima_compra"] == "bajo":
            textosA.append("Más activos: han comprado recientemente.")
        else:
            textosA.append("Actividad intermedia: recencia promedio.")

        # 5) Diversidad
        if tags["numero_productos_distintos"] == "alto":
            textosA.append("Compran variedad alta de productos.")
        elif tags["numero_productos_distintos"] == "bajo":
            textosA.append("Compran poca variedad de productos.")
        else:
            textosA.append("Variedad de productos promedio.")

        # 6) Mini lectura (solo 3 casos, sencillo)
        gasto = tags["monto_total_gastado"]
        freq = tags["frecuencia_compra"]
        rec = tags["dias_desde_ultima_compra"]

        if gasto == "alto" and freq == "alto" and rec == "bajo":
            textosA.append("Lectura rápida: segmento valioso y frecuente, con actividad reciente.")
        elif rec == "alto":
            textosA.append("Lectura rápida: segmento con riesgo de inactividad.")
        elif gasto == "bajo" and freq == "alto":
            textosA.append("Lectura rápida: compran seguido pero con tickets bajos.")
        else:
            textosA.append("Lectura rápida: comportamiento balanceado.")

        # Formato final
        header = f"Cluster {int(c)}:"
        cuerpo = "\n".join([f"\t• {b}" for b in textosA])
        bloques.append(f"{header}\n{cuerpo}")

    return "\n\n".join(bloques)

# ===== USO =====
texto_clusters = generar_descripcion_listada_simple(df)
print(texto_clusters)

# =========================
# 6. CLUSTERING (TEXTO / RESEÑAS)
# =========================

import numpy as np
import pandas as pd

from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize
from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

# 1) Corpus limpio
text_series = df["texto_reseña"].astype(str).fillna("").str.strip()
text_series = text_series[text_series != ""]
df_text = df.loc[text_series.index].copy()
corpus = text_series.tolist()

# 2) Modelo de sentence embeddings (multilingüe, ligero y bueno)
model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

# 3) Embeddings (batching para eficiencia)
emb = model.encode(
    corpus,
    batch_size=64,
    show_progress_bar=True,
    convert_to_numpy=True,
    normalize_embeddings=True  # deja embeddings listos para similitud coseno
)

# 4) Clustering eficiente
k_text = 4
kmeans = MiniBatchKMeans(
    n_clusters=k_text,
    random_state=42,
    batch_size=2048,
    max_iter=200,
    n_init="auto"
)
labels = kmeans.fit_predict(emb)
df_text["cluster_reseñas"] = labels

# 5) Métricas (ya es denso y pequeño -> rápido)
# Silhouette: si son muchísimas reseñas, muestrea
n = emb.shape[0]
sample_size = min(5000, n)
if sample_size < n:
    rng = np.random.RandomState(42)
    idx = rng.choice(n, size=sample_size, replace=False)
    sil = silhouette_score(emb[idx], labels[idx], metric="cosine")
else:
    sil = silhouette_score(emb, labels, metric="cosine")

ch = calinski_harabasz_score(emb, labels)
db = davies_bouldin_score(emb, labels)
inercia = kmeans.inertia_

print("=== RESEÑAS (Sentence Embeddings + MiniBatchKMeans) ===")
print("K =", k_text)
print("Inercia:", inercia)
print("Silhouette (cosine, sample):", sil)
print("Calinski-Harabasz:", ch)
print("Davies-Bouldin:", db)

# 6) Guardar en df original
df.loc[df_text.index, "cluster_reseñas"] = df_text["cluster_reseñas"]

df

# Tamaño y porcentaje de clusters de reseñas
cluster_size_text = (
    df_text["cluster_reseñas"]
    .value_counts()
    .sort_index()
    .rename("cantidad")
    .to_frame()
)

cluster_size_text["porcentaje"] = (
    cluster_size_text["cantidad"] / cluster_size_text["cantidad"].sum() * 100
)

cluster_size_text

import numpy as np
import pandas as pd

def top_comentarios_cercanos_unicos(
    df_text: pd.DataFrame,
    emb: np.ndarray,
    labels: np.ndarray,
    col_texto: str = "texto_reseña",
    top_n: int = 5
) -> pd.DataFrame:
    """
    Devuelve una tabla con los comentarios más cercanos al centroide
    (más representativos) por cluster, evitando textos repetidos.
    """
    labels = np.asarray(labels)
    n_clusters = labels.max() + 1

    filas = []
    textos = df_text[col_texto].astype(str).fillna("").to_numpy()

    for c in range(n_clusters):
        idx = np.where(labels == c)[0]
        if len(idx) == 0:
            continue

        # Centroide normalizado
        centroid = emb[idx].mean(axis=0)
        centroid /= np.linalg.norm(centroid)

        # Similitud coseno
        sims = emb[idx] @ centroid

        # Ordenar por similitud descendente
        order = np.argsort(-sims)

        vistos = set()
        count = 0

        for j in order:
            i = idx[j]
            texto = textos[i].strip()

            if texto.lower() in vistos:
                continue

            vistos.add(texto.lower())

            filas.append({
                "cluster": int(c),
                "similitud_coseno": float(sims[j]),
                "comentario_representativo": texto
            })

            count += 1
            if count >= top_n:
                break

    return pd.DataFrame(filas)

# ===== USO =====
tabla_top5 = top_comentarios_cercanos_unicos(
    df_text=df_text,
    emb=emb,
    labels=labels,
    col_texto="texto_reseña",
    top_n=5
)

tabla_top5

import numpy as np
import pandas as pd

def coherencia_por_cluster(emb, labels):
    resultados = []
    labels = np.asarray(labels)
    n_clusters = labels.max() + 1

    for c in range(n_clusters):
        idx = np.where(labels == c)[0]
        if len(idx) == 0:
            continue

        centroid = emb[idx].mean(axis=0)
        centroid /= np.linalg.norm(centroid)

        sims = emb[idx] @ centroid
        resultados.append({
            "cluster": int(c),
            "coherencia_media": float(sims.mean()),
            "coherencia_min": float(sims.min()),
            "coherencia_max": float(sims.max()),
            "cantidad": len(idx)
        })

    return pd.DataFrame(resultados)

tabla_coherencia = coherencia_por_cluster(emb, labels)
tabla_coherencia

cluster_size_text["porcentaje"].plot(
    kind="bar",
    title="Distribución porcentual de reseñas por cluster"
)
plt.ylabel("Porcentaje (%)")
plt.xlabel("Cluster")
plt.show()