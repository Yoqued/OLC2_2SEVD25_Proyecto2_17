# -*- coding: utf-8 -*-
"""PY2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15LgrzyPaufvUM-K4Tif0K46gItTbvSn7
"""

import pandas as pd
import numpy as np

# Leer el archivo CSV
df = pd.read_csv("data_prueba_proyecto2.csv")

df.info()
df.describe()

# 1. TIPO CORRECTO DE LOS DATOS

# Columnas numéricas para el análisis
cols_numericas = [
    'frecuencia_compra',
    'monto_total_gastado',
    'monto_promedio_compra',
    'dias_desde_ultima_compra',
    'antiguedad_cliente_meses',
    'numero_productos_distintos'
]

# Convertir columnas a numérico (errores → NaN)
for col in cols_numericas:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# Convertir columnas a string
df['canal_principal'] = df['canal_principal'].astype("string")
df['texto_reseña'] = df['texto_reseña'].astype("string")
df['producto_categoria'] = df['producto_categoria'].astype("string")

# 2. ELIMINACIÓN DE DUPLICADOS POR cliente_id y reseña_id

df = df.drop_duplicates(subset=[
    'cliente_id',
    'reseña_id'
])

# 3. LIMPIEZA DE VARIABLES

# FRECUENCIA COMPRA

df.loc[df['frecuencia_compra'] < 0, 'frecuencia_compra'] = np.nan

q1 = df['frecuencia_compra'].quantile(0.25)
q3 = df['frecuencia_compra'].quantile(0.75)
iqr = q3 - q1

limite_superior = q3 + 1.5 * iqr

df.loc[df['frecuencia_compra'] > limite_superior, 'frecuencia_compra'] = np.nan

# 3. LIMPIEZA DE VARIABLES

# MONTO TOTAL GASTADO

df.loc[df['monto_total_gastado'] < 0, 'monto_total_gastado'] = np.nan

q1 = df['monto_total_gastado'].quantile(0.25)
q3 = df['monto_total_gastado'].quantile(0.75)
iqr = q3 - q1

limite_superior = q3 + 3.0 * iqr

df.loc[df['monto_total_gastado'] > limite_superior, 'monto_total_gastado'] = np.nan

# 3. LIMPIEZA DE VARIABLES

# MONTO PROMEDIO COMPRA

df.loc[df['monto_promedio_compra'] < 0, 'monto_promedio_compra'] = np.nan

q1 = df['monto_promedio_compra'].quantile(0.25)
q3 = df['monto_promedio_compra'].quantile(0.75)
iqr = q3 - q1

limite_superior = q3 + 1.5 * iqr

df.loc[df['monto_promedio_compra'] > limite_superior, 'monto_promedio_compra'] = np.nan

# 3. LIMPIEZA DE VARIABLES

# DIAS DESDE ULTIMA COMPRA

df.loc[df['dias_desde_ultima_compra'] < 0, 'dias_desde_ultima_compra'] = np.nan

p95 = df["dias_desde_ultima_compra"].quantile(0.95)

df.loc[df["dias_desde_ultima_compra"] > p95, "dias_desde_ultima_compra"] = p95

# 3. LIMPIEZA DE VARIABLES

# ANTIGUEDAD CLIENTE MESES

df.loc[df['antiguedad_cliente_meses'] < 0, 'antiguedad_cliente_meses'] = np.nan

q1 = df['antiguedad_cliente_meses'].quantile(0.25)
q3 = df['antiguedad_cliente_meses'].quantile(0.75)
iqr = q3 - q1

limite_superior = q3 + 3 * iqr

df.loc[
    df['antiguedad_cliente_meses'] > limite_superior,
    'antiguedad_cliente_meses'
] = np.nan

# 3. LIMPIEZA DE VARIABLES

# CANAL PRINCIPAL

df['canal_principal'] = df['canal_principal'].str.strip().str.lower()

# 3. LIMPIEZA DE VARIABLES

# NUMERO PRODUCTOS DISTINTOS

df.loc[df['numero_productos_distintos'] < 0, 'numero_productos_distintos'] = np.nan

# 2. Detección de outliers (IQR - límite superior)
q1 = df['numero_productos_distintos'].quantile(0.25)
q3 = df['numero_productos_distintos'].quantile(0.75)
iqr = q3 - q1

limite_superior = q3 + 1.5 * iqr

df.loc[
    df['numero_productos_distintos'] > limite_superior,
    'numero_productos_distintos'
] = np.nan

# 3. LIMPIEZA DE VARIABLES

# RESEÑA ID

df['reseña_id'] = pd.to_numeric(df['reseña_id'], errors='coerce')

df.loc[df['reseña_id'] <= 0, 'reseña_id'] = np.nan

# 3. LIMPIEZA DE VARIABLES

# TEXTO RESEÑA

df['texto_reseña'] = (
    df['texto_reseña']
    .str.strip()
    .str.lower()
)
df = df[df['texto_reseña'].notna() & (df['texto_reseña'] != "")]

# 3. LIMPIEZA DE VARIABLES

# FECHA RESEÑA

df['fecha_reseña'] = pd.to_datetime(df['fecha_reseña'], errors='coerce')

# 3. LIMPIEZA DE VARIABLES

# PRODUCTO CATEGORIA

df['producto_categoria'] = df['producto_categoria'].str.strip().str.lower()

# 3. LIMPIEZA DE VARIABLES

# LONGITUD RESEÑA

df['longitud_reseña'] = df['texto_reseña'].str.split().str.len()

df['longitud_reseña'] = pd.to_numeric(
    df['longitud_reseña'],
    errors='coerce'
)

# 4. IMPUTACIÓN O ELIMINACIÓN

# CLIENTE ID

df = df[df['cliente_id'].notna()]

# 4. IMPUTACIÓN O ELIMINACIÓN

# FRECUENCIA COMPRA

mediana_frecuencia = df['frecuencia_compra'].median()
df['frecuencia_compra'] = df['frecuencia_compra'].fillna(mediana_frecuencia)

# 4. IMPUTACIÓN O ELIMINACIÓN

# MONTO TOTAL GASTADO

mediana_monto_total = df['monto_total_gastado'].median()
df['monto_total_gastado'] = df['monto_total_gastado'].fillna(mediana_monto_total)

# 4. IMPUTACIÓN O ELIMINACIÓN

# MONTO PROMEDIO COMPRA

mediana_monto_promedio = df['monto_promedio_compra'].median()
df['monto_promedio_compra'] = df['monto_promedio_compra'].fillna(mediana_monto_promedio)

# 4. IMPUTACIÓN O ELIMINACIÓN

# DIAS DESDE ULTIMA COMPRA

mediana_dias = df['dias_desde_ultima_compra'].median()
df['dias_desde_ultima_compra'] = df['dias_desde_ultima_compra'].fillna(mediana_dias)

# 4. IMPUTACION O ELIMINACIÓN

# ANTIGUEDAD CLIENTE MESES

mediana_antiguedad = df['antiguedad_cliente_meses'].median()
df['antiguedad_cliente_meses'] = df['antiguedad_cliente_meses'].fillna(mediana_antiguedad)

# 4. IMPUTACION O ELIMINACIÓN

# NUMERO PRODUCTOS DISTINTOS

mediana_productos = df['numero_productos_distintos'].median()
df['numero_productos_distintos'] = df['numero_productos_distintos'].fillna(mediana_productos)

# 4. IMPUTACION O ELIMINACIÓN

# RESEÑA ID

df = df[df['reseña_id'].notna()]

df.info()
df.describe()

"""# CLUSTERING (NUMÉRICO)"""

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import (
    silhouette_score,
    calinski_harabasz_score,
    davies_bouldin_score
)

# Columnas numéricas que usarás para clustering de clientes
cols_cluster_numerico = [
    'frecuencia_compra',
    'monto_total_gastado',
    'monto_promedio_compra',
    'dias_desde_ultima_compra',
    'antiguedad_cliente_meses',
    'numero_productos_distintos'
]

# Matriz numérica
X_num = df[cols_cluster_numerico].copy()

# Normalización (importante para K-Means)
scaler = StandardScaler()
X_num_scaled = scaler.fit_transform(X_num)

# Elegir K (manual)
k_num = 6

kmeans_num = KMeans(n_clusters=k_num, random_state=42, n_init=10)
clusters_num = kmeans_num.fit_predict(X_num_scaled)

df['cluster_clientes'] = clusters_num

# Métricas
sil_num = silhouette_score(X_num_scaled, clusters_num)
inercia_num = kmeans_num.inertia_
ch_num = calinski_harabasz_score(X_num_scaled, clusters_num)
db_num = davies_bouldin_score(X_num_scaled, clusters_num)

print("=== CLUSTERING NUMÉRICO (CLIENTES) ===")
print("K (manual) =", k_num)
print("Silhouette:", sil_num)
print("Inercia:", inercia_num)
print("Calinski-Harabasz:", ch_num)
print("Davies-Bouldin:", db_num)

# Perfil rápido por cluster (promedios por grupo)
perfil_clientes = df.groupby('cluster_clientes')[cols_cluster_numerico].mean()
print("\nPerfil por cluster (promedios):")
print(perfil_clientes)

# =========================
# 6. CLUSTERING (TEXTO / RESEÑAS)
# =========================

import re
from sklearn.feature_extraction.text import TfidfVectorizer

# Asegurar texto (por seguridad)
df['texto_reseña'] = df['texto_reseña'].astype(str)

# Normalización ligera de texto (para TF-IDF)
def normalizar_texto(s: str) -> str:
    s = s.lower().strip()
    s = re.sub(r'\d+', ' ', s)          # quitar números
    s = re.sub(r'[^\w\s]', ' ', s)      # quitar puntuación
    s = re.sub(r'\s+', ' ', s).strip()  # espacios extra
    return s

df['texto_reseña_norm'] = df['texto_reseña'].apply(normalizar_texto)

# Vectorización TF-IDF (con n-gramas 1-2, muy útil en reseñas)
tfidf = TfidfVectorizer(
    stop_words=None,      # luego podemos poner stopwords en español si quieres
    ngram_range=(1, 2),
    min_df=2              # si tu dataset es pequeño, cambia a 1
)

X_text = tfidf.fit_transform(df['texto_reseña_norm'])

# Elegir K para reseñas (manual)
k_text = 5

kmeans_text = KMeans(n_clusters=k_text, random_state=42, n_init=10)
clusters_text = kmeans_text.fit_predict(X_text)

df['cluster_resenas'] = clusters_text

# Métricas
sil_text = silhouette_score(X_text, clusters_text)
inercia_text = kmeans_text.inertia_
ch_text = calinski_harabasz_score(X_text, clusters_text)
db_text = davies_bouldin_score(X_text, clusters_text)

# "k" asociado a cada métrica (mismo k elegido)
k_silueta_text = k_text
k_inercia_text = k_text

print("\n=== CLUSTERING TEXTO (RESEÑAS) ===")
print("K (manual) =", k_text)
print("K (Silueta) =", k_silueta_text, "| Silhouette:", sil_text)
print("K (Inercia) =", k_inercia_text, "| Inercia:", inercia_text)
print("Calinski-Harabasz:", ch_text)
print("Davies-Bouldin:", db_text)

# Extra: palabras/frases más representativas por cluster
feature_names = tfidf.get_feature_names_out()
centers = kmeans_text.cluster_centers_

top_n = 10
print("\nTop términos por cluster (reseñas):")
for i in range(k_text):
    top_idx = centers[i].argsort()[-top_n:][::-1]
    top_terms = [feature_names[j] for j in top_idx]
    print(f"Cluster {i}: {top_terms}")


# =========================
# 7. GUARDAR RESULTADOS
# =========================

df.to_csv("clientes_con_clusters.csv", index=False)
print("\nArchivo guardado: clientes_con_clusters.csv")